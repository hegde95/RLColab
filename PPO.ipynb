{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PPO.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "4_O686GTsbdq",
        "oK6AnnC43_-B",
        "bq1Li-Wu8yRc",
        "BytWHGFqlpoM",
        "nmnHEJ-xmBlP"
      ],
      "authorship_tag": "ABX9TyO8TvJN2Zv7SiLM7zXUyvYK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hegde95/RLColab/blob/master/PPO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FwFWtezKiegg",
        "colab_type": "text"
      },
      "source": [
        "# Introduction\n",
        "This Colab notebook trains a PPO agent on the chosen basic gym environment. This notebook is a part of the following github project:\n",
        "[https://github.com/hegde95/RLColab/](https://github.com/hegde95/RLColab/)\n",
        "\n",
        "The main aim of this notebook is to help RL enthusiasts (such as my self), not be limited by hardware.\n",
        "\n",
        "![alt text](https://media.giphy.com/media/5Zesu5VPNGJlm/giphy.gif)\n",
        "\n",
        "This notebook just trains an agent (without rendering anything), and saves the agent in the provided drive folder. To test the agent, run the play.py python script from the github project.\n",
        "\n",
        "## Steps for this project:\n",
        "\n",
        "1.   Clone this repo:\n",
        ">```\n",
        "$ git clone https://github.com/hegde95/RLColab.git\n",
        "```    \n",
        "\n",
        "2.   Add the lib, runs and checkpoints folders from the repo to your Google Drive. The checkpoints folder is the place where the models will be saved.\n",
        "\n",
        "3.   Configure Section 3 as needed and run this CoLab Notebook for as long as needed. Follow the instructions below to run this notebook. Since google can terminate your session at any time, the models are saved to the checkpoints folder whenever the test rewards improve. Therefore you can choose to load a previously trained model in Section 3. \n",
        "\n",
        "\n",
        "4.   Download the needed model (.dat file) from the checkpoints folder on drive to the checkpoints folder on your local repo.\n",
        "\n",
        "5.   Test the model by running play.py\n",
        ">Esure you have the following dependencies installed (gym, torch) on your local machine.\n",
        "```\n",
        "pip install gym\n",
        "pip install torch\n",
        "```\n",
        "run play.py\n",
        "```\n",
        "python play.py\n",
        "```\n",
        "Note that this only tests the last model that was downloaded to the local checkpoints folder.\n",
        "\n",
        "\n",
        "####So Lets Start\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tvhnA9JHK4f",
        "colab_type": "text"
      },
      "source": [
        "## Takeoff!!! \n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Before running make sure that GPU is enabled:\n",
        "Edit -> Notebook settings -> Hardware accelerator dropdown choose GPU\n",
        "\n",
        "To run the notebook:\n",
        "Runtime -> Run all\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Add this code in inspector to keep colab from restarting.\n",
        "\n",
        "Ctrl+ Shift + i to open inspector view . Then goto console and run the following\n",
        "\n",
        "\n",
        "```\n",
        "function ClickConnect(){\n",
        "console.log(\"Working\"); \n",
        "document.querySelector(\"colab-toolbar-button#connect\").click() \n",
        "}\n",
        "setInterval(ClickConnect,60000)\n",
        "```\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_O686GTsbdq",
        "colab_type": "text"
      },
      "source": [
        "# Section 1: Get Current GPU data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdoCXBarsZxX",
        "colab_type": "code",
        "colab": {},
        "cellView": "both"
      },
      "source": [
        "#@title\n",
        "# memory footprint support libraries/code\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "!nvidia-smi\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isnâ€™t guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        " process = psutil.Process(os.getpid())\n",
        " print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        " print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "printm()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oK6AnnC43_-B",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# Section 2: Set up and imports\n",
        "Will get stuck here everytime the session restarts. Expand this section and authenticate google to access your drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhBC9SKqYhK1",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title # Add Google Drive path here{ run: \"auto\" }\n",
        "import sys\n",
        "#@markdown Enter the path like so: /content/drive/My Drive/RL/PPO/\n",
        "path_to_project = '/content/drive/My Drive/RL/PPO1/' #@param {type:\"string\"}\n",
        "sys.path.append(path_to_project)\n",
        "#@markdown This Folder should be set up in your Google drive and must contain the lib, checkpoints and runs folders"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBqkdLxvBsAQ",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title ##Connect to drive\n",
        "#@markdown ###please authenticate if needed\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38jEV3-xB_Z0",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title Install tensorboardX and box2d-py\n",
        "!pip install tensorboardX\n",
        "!pip install box2d-py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_-gkLhaBOwu",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title Imports\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import os\n",
        "import gym\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "\n",
        "import os\n",
        "from tensorboardX import SummaryWriter\n",
        "import warnings\n",
        "\n",
        "\n",
        "from lib.common import mkdir\n",
        "from lib.Model import ActorCritic\n",
        "from lib.multiprocessing_env import SubprocVecEnv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bq1Li-Wu8yRc",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# Section 3: Variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6Zd5RaICKNj",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title # Configure Hyper-parameters here{ run: \"auto\" }\n",
        "\n",
        "NUM_ENVS = 8 #@param {type:\"integer\"}\n",
        "ENV_ID = \"BipedalWalker-v3\" #@param {type:\"string\"}\n",
        "HIDDEN_SIZE = 256 #@param {type:\"integer\"}\n",
        "LEARNING_RATE = 1e-4 #@param {type:\"number\"}\n",
        "GAMMA = 0.99 #@param {type:\"number\"}\n",
        "GAE_LAMBDA = 0.95 #@param {type:\"number\"}\n",
        "PPO_EPSILON = 0.2 #@param {type:\"number\"}\n",
        "CRITIC_DISCOUNT = 0.5 #@param {type:\"number\"}\n",
        "ENTROPY_BETA = 0.001 #@param {type:\"number\"}\n",
        "PPO_STEPS = 1024 #@param {type:\"integer\"}\n",
        "MINI_BATCH_SIZE = 64 #@param {type:\"integer\"}\n",
        "PPO_EPOCHS = 10 #@param {type:\"integer\"}\n",
        "TEST_EPOCHS = 10 #@param {type:\"integer\"}\n",
        "NUM_TESTS = 5 #@param {type:\"integer\"}\n",
        "TARGET_REWARD = 2500 #@param {type:\"integer\"}\n",
        "\n",
        "#@markdown ------\n",
        "\n",
        "#@markdown If you have a previously trained model for the same ENV_ID and HIDDEN_SIZE, you may choose Latest, Best or Custom\n",
        "\n",
        "#@markdown Select New, If you want to create a new model\n",
        "\n",
        "#@markdown Select Latest, If you want to choose the latest model\n",
        "\n",
        "#@markdown Select Best, If you want to choose the best model\n",
        "\n",
        "#@markdown Select Custom, If you want to load a custom model\n",
        "\n",
        "#@markdown If running for the first time, choose New\n",
        "\n",
        "LOAD_MODEL = \"New\"  #@param [\"New\", \"Custom\", \"Latest\", \"Best\"]\n",
        "#@markdown NOTE: If you selected an option other than New, MAKE SURE THE MODEL ARCHITECTURE IS NOT DIFFERENT\n",
        "\n",
        "#@markdown If you selected Custom for LOAD_MODEL, enter the name of the model to be restored.\n",
        "\n",
        "import glob\n",
        "import sys\n",
        "\n",
        "def getScore(s):\n",
        "    c= re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", s)\n",
        "    return float(c[-2])\n",
        "\n",
        "if LOAD_MODEL == \"Latest\":\n",
        "    list_of_files = glob.glob(path_to_project+'checkpoints/*')\n",
        "    model_name = max(list_of_files, key=os.path.getctime)\n",
        "    print('Loading the following file:')\n",
        "    print(model_name)\n",
        "elif LOAD_MODEL == \"Best\":\n",
        "    list_of_files = glob.glob(path_to_project+'checkpoints/*')\n",
        "    bs = -99999\n",
        "    model_name = \"\"\n",
        "    for model in list_of_files:\n",
        "        if getScore(model)>bs:\n",
        "            model_name = model\n",
        "            bs = getScore(model)\n",
        "    print('Loading the following file:')\n",
        "    print(model_name)\n",
        "elif LOAD_MODEL == \"Custom\":\n",
        "    CUSTOM_MODEL_NAME = \"BipedalWalker-v3_best_+12.372_20480.dat\" #@param {type:\"string\"}  \n",
        "    model_name = CUSTOM_MODEL_NAME\n",
        "    list_of_files = glob.glob(path_to_project+'checkpoints/*')\n",
        "    if path_to_project+'checkpoints/'+model_name in list_of_files:\n",
        "        print('Loading the following file:')\n",
        "        print(model_name)\n",
        "    else:\n",
        "        print('Model not found')\n",
        "        sys.exit(\"Model Not Found\")\n",
        "else:\n",
        "    print('Creating new model')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BytWHGFqlpoM",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# Section 4: PPO Methods"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8r35ra7ZCT-B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Based on https://github.com/higgsfield/RL-Adventure-2/blob/master/3.ppo.ipynb\n",
        "# Based on https://github.com/colinskow/move37/blob/master/ppo/ppo_train.py\n",
        "\n",
        "def make_env():\n",
        "    # returns a function which creates a single environment\n",
        "    def _thunk():\n",
        "        env = gym.make(ENV_ID)\n",
        "        return env\n",
        "    return _thunk\n",
        "\n",
        "\n",
        "def test_env(env, model, device, deterministic=True):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "    i = 0\n",
        "    while (not done) and (i<1024):\n",
        "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
        "        dist, _ = model(state)\n",
        "        action = dist.mean.detach().cpu().numpy()[0] if deterministic \\\n",
        "            else dist.sample().cpu().numpy()[0]\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "        i +=1\n",
        "    return total_reward\n",
        "\n",
        "\n",
        "def normalize(x):\n",
        "    x -= x.mean()\n",
        "    x /= (x.std() + 1e-8)\n",
        "    return x\n",
        "\n",
        "\n",
        "def compute_gae(next_value, rewards, masks, values, gamma=GAMMA, lam=GAE_LAMBDA):\n",
        "    values = values + [next_value]\n",
        "    gae = 0\n",
        "    returns = []\n",
        "    for step in reversed(range(len(rewards))):\n",
        "        delta = rewards[step] + gamma * \\\n",
        "            values[step + 1] * masks[step] - values[step]\n",
        "        gae = delta + gamma * lam * masks[step] * gae\n",
        "        # prepend to get correct order back\n",
        "        returns.insert(0, gae + values[step])\n",
        "    return returns\n",
        "\n",
        "\n",
        "def ppo_iter(states, actions, log_probs, returns, advantage):\n",
        "    batch_size = states.size(0)\n",
        "    # generates random mini-batches until we have covered the full batch\n",
        "    for _ in range(batch_size // MINI_BATCH_SIZE):\n",
        "        rand_ids = np.random.randint(0, batch_size, MINI_BATCH_SIZE)\n",
        "        yield states[rand_ids, :], actions[rand_ids, :], log_probs[rand_ids, :], returns[rand_ids, :], advantage[rand_ids, :]\n",
        "\n",
        "\n",
        "def ppo_update(frame_idx, states, actions, log_probs, returns, advantages, clip_param=PPO_EPSILON):\n",
        "    count_steps = 0\n",
        "    sum_returns = 0.0\n",
        "    sum_advantage = 0.0\n",
        "    sum_loss_actor = 0.0\n",
        "    sum_loss_critic = 0.0\n",
        "    sum_entropy = 0.0\n",
        "    sum_loss_total = 0.0\n",
        "\n",
        "    # PPO EPOCHS is the number of times we will go through ALL the training data to make updates\n",
        "    for _ in range(PPO_EPOCHS):\n",
        "        # grabs random mini-batches several times until we have covered all data\n",
        "        for state, action, old_log_probs, return_, advantage in ppo_iter(states, actions, log_probs, returns, advantages):\n",
        "            dist, value = model(state)\n",
        "            entropy = dist.entropy().mean()\n",
        "            new_log_probs = dist.log_prob(action)\n",
        "\n",
        "            ratio = (new_log_probs - old_log_probs).exp()\n",
        "            surr1 = ratio * advantage\n",
        "            surr2 = torch.clamp(ratio, 1.0 - clip_param,\n",
        "                                1.0 + clip_param) * advantage\n",
        "\n",
        "            actor_loss = - torch.min(surr1, surr2).mean()\n",
        "            critic_loss = (return_ - value).pow(2).mean()\n",
        "            loss = CRITIC_DISCOUNT * critic_loss + actor_loss - ENTROPY_BETA * entropy\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "\n",
        "            # track statistics\n",
        "            sum_returns += return_.mean()\n",
        "            sum_advantage += advantage.mean()\n",
        "            sum_loss_actor += actor_loss\n",
        "            sum_loss_critic += critic_loss\n",
        "            sum_loss_total += loss\n",
        "            sum_entropy += entropy\n",
        "\n",
        "            count_steps += 1\n",
        "\n",
        "    writer.add_scalar(\"returns\", sum_returns / count_steps, frame_idx)\n",
        "    writer.add_scalar(\"advantage\", sum_advantage / count_steps, frame_idx)\n",
        "    writer.add_scalar(\"loss_actor\", sum_loss_actor / count_steps, frame_idx)\n",
        "    writer.add_scalar(\"loss_critic\", sum_loss_critic / count_steps, frame_idx)\n",
        "    writer.add_scalar(\"entropy\", sum_entropy / count_steps, frame_idx)\n",
        "    writer.add_scalar(\"loss_total\", sum_loss_total / count_steps, frame_idx)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmnHEJ-xmBlP",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# Section 5: Main Method\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6CQpXG7Cltp",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title\n",
        "runs = str(path_to_project) + \"runs/\"\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir \"$runs\"\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "writer = SummaryWriter(path_to_project+'runs/'+str(datetime.now())+'/',comment=\"ppo_\" + \"AlienGo\")\n",
        "device = torch.device(\"cuda\")\n",
        "print('Device:', device)\n",
        "# Prepare environments\n",
        "envs = [make_env() for i in range(NUM_ENVS)]\n",
        "envs = SubprocVecEnv(envs)\n",
        "env = gym.make(ENV_ID)\n",
        "\n",
        "\n",
        "num_inputs = envs.observation_space.shape[0]\n",
        "num_outputs = envs.action_space.shape[0]\n",
        "\n",
        "model = ActorCritic(num_inputs, num_outputs, HIDDEN_SIZE).to(device)\n",
        "if LOAD_MODEL != \"New\":\n",
        "    model.load_state_dict(torch.load(model_name))\n",
        "    print(\"Loaded the file:\"+model_name)\n",
        "\n",
        "print(model)\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "frame_idx = 0\n",
        "train_epoch = 0\n",
        "if LOAD_MODEL != \"New\":\n",
        "    best_reward = getScore(model_name)\n",
        "else:\n",
        "    best_reward = -9999\n",
        "\n",
        "state = envs.reset()\n",
        "early_stop = False\n",
        "while not early_stop:\n",
        "\n",
        "    log_probs = []\n",
        "    values = []\n",
        "    states = []\n",
        "    actions = []\n",
        "    rewards = []\n",
        "    masks = []\n",
        "\n",
        "    for _ in range(PPO_STEPS):\n",
        "        state = torch.FloatTensor(state).to(device)\n",
        "        dist, value = model(state)\n",
        "\n",
        "        action = dist.sample()\n",
        "        # each state, reward, done is a list of results from each parallel environment\n",
        "        next_state, reward, done, _ = envs.step(action.cpu().numpy())\n",
        "        log_prob = dist.log_prob(action)\n",
        "\n",
        "        log_probs.append(log_prob)\n",
        "        values.append(value)\n",
        "        rewards.append(torch.FloatTensor(reward).unsqueeze(1).to(device))\n",
        "        masks.append(torch.FloatTensor(1 - done).unsqueeze(1).to(device))\n",
        "\n",
        "        states.append(state)\n",
        "        actions.append(action)\n",
        "\n",
        "        state = next_state\n",
        "        frame_idx += 1\n",
        "\n",
        "    next_state = torch.FloatTensor(next_state).to(device)\n",
        "    _, next_value = model(next_state)\n",
        "    returns = compute_gae(next_value, rewards, masks, values)\n",
        "\n",
        "    returns = torch.cat(returns).detach()\n",
        "    log_probs = torch.cat(log_probs).detach()\n",
        "    values = torch.cat(values).detach()\n",
        "    states = torch.cat(states)\n",
        "    actions = torch.cat(actions)\n",
        "    advantage = returns - values\n",
        "    advantage = normalize(advantage)\n",
        "\n",
        "    ppo_update(frame_idx, states, actions, log_probs, returns, advantage)\n",
        "    train_epoch += 1\n",
        "\n",
        "    if train_epoch % TEST_EPOCHS == 0:\n",
        "        test_reward = np.mean([test_env(env, model, device)\n",
        "                                for _ in range(NUM_TESTS)])\n",
        "        writer.add_scalar(\"test_rewards\", test_reward, frame_idx)\n",
        "        print('Frame %s. reward: %s' % (frame_idx, test_reward))\n",
        "        # Save a checkpoint every time we achieve a best reward\n",
        "        if best_reward is None or best_reward < test_reward:\n",
        "            if best_reward is not None:\n",
        "                print(\"Best reward updated: %.3f -> %.3f\" %\n",
        "                      (best_reward, test_reward))\n",
        "                name = \"%s_best_%+.3f_%d.dat\" % (ENV_ID,\n",
        "                                                  test_reward, frame_idx)\n",
        "                fname = os.path.join('.', path_to_project+'checkpoints', name)\n",
        "                torch.save(model.state_dict(), fname)\n",
        "            best_reward = test_reward\n",
        "        if test_reward > TARGET_REWARD:\n",
        "            early_stop = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3K2hvgctipS",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "# Landing:\n",
        "This Notebook was just something I tried out of curiosity and is nowhere close to where cutting edge RL research is. But it can be extended to fit the use case.\n",
        "\n",
        "Future work:\n",
        "\n",
        "\n",
        "*   Implement Custom Environments. These gym wrapper classes can be placed in the same drive folder and can be imported from this notebook\n",
        "*   Try using TPU's for RL\n",
        "\n",
        "\n",
        "If you like this work give me a shoutout :)\n",
        "\n",
        "![alt text](https://media.giphy.com/media/ui1hpJSyBDWlG/giphy.gif)\n",
        "\n",
        "website: https://hegde95.github.io/\n",
        "\n",
        "LinkedIn: https://www.linkedin.com/in/karkala-shashank-hegde/\n",
        "\n",
        "\n"
      ]
    }
  ]
}